{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FD6IzcpKCbQx",
        "outputId": "f68bc7fc-108b-431d-d3dc-914abd5fe4f8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf, sys\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(\"GPU available:\", len(gpus) > 0, \"| GPU devices:\", gpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "collapsed": true,
        "id": "M-Is_IDWCoXd",
        "outputId": "10228929-b4de-461a-9268-570b319e796b"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "import os, shutil, glob\n",
        "\n",
        "print(\"Upload your Kaggle token file (e.g. 'kaggle (4).json').\")\n",
        "uploaded = files.upload()\n",
        "kaggle_file = None\n",
        "for fname in uploaded.keys():\n",
        "    if 'kaggle' in fname.lower() and fname.lower().endswith('.json'):\n",
        "        kaggle_file = fname\n",
        "        break\n",
        "\n",
        "if kaggle_file is None:\n",
        "    raise RuntimeError(\"No kaggle json uploaded. Please upload the file 'kaggle (4).json' (or any kaggle .json).\")\n",
        "\n",
        "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "os.makedirs(kaggle_dir, exist_ok=True)\n",
        "dest = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "\n",
        "if os.path.exists(dest):\n",
        "    os.remove(dest)\n",
        "shutil.move(kaggle_file, dest)\n",
        "os.chmod(dest, 0o600)\n",
        "print(\"Kaggle token installed to ~/.kaggle/kaggle.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uhHsnBUBC8X7",
        "outputId": "469311ef-5353-4453-b2e5-4396ff955d1f"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_slug = \"gunavenkatdoddi/eye-diseases-classification\"\n",
        "\n",
        "\n",
        "if os.path.exists(\"eye-diseases-classification.zip\"):\n",
        "    os.remove(\"eye-diseases-classification.zip\")\n",
        "\n",
        "print(\"Downloading dataset:\", dataset_slug, \" — this may take a moment.\")\n",
        "\n",
        "!kaggle datasets download -d {dataset_slug} -q || echo \"kaggle download failed — check token/permission.\"\n",
        "\n",
        "\n",
        "zip_files = glob.glob(\"/content/*.zip\")\n",
        "if not zip_files:\n",
        "    raise RuntimeError(\"No zip file found in /content. Check Kaggle download step output above.\")\n",
        "zip_path = zip_files[0]\n",
        "print(\"Found zip:\", zip_path)\n",
        "\n",
        "\n",
        "import zipfile\n",
        "extract_root = \"/content/dataset_extracted\"\n",
        "if os.path.exists(extract_root):\n",
        "    shutil.rmtree(extract_root)\n",
        "os.makedirs(extract_root, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_root)\n",
        "print(\"Extracted dataset to:\", extract_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0MJms9jDS_S",
        "outputId": "4157651e-e3df-4521-fc59-ba38e826047c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "def contains_image_files(path):\n",
        "    try:\n",
        "        for f in os.listdir(path):\n",
        "            if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                return True\n",
        "    except Exception:\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "def find_dataset_dir(root):\n",
        "\n",
        "    children = [os.path.join(root, d) for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))]\n",
        "\n",
        "    img_count = sum(1 for c in children if contains_image_files(c))\n",
        "    if img_count >= 2:\n",
        "        return root\n",
        "\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        child_dirs = [os.path.join(dirpath, d) for d in os.listdir(dirpath) if os.path.isdir(os.path.join(dirpath,d))]\n",
        "        cnt = 0\n",
        "        for s in child_dirs:\n",
        "            if contains_image_files(s):\n",
        "                cnt += 1\n",
        "        if cnt >= 2:\n",
        "            return dirpath\n",
        "    return None\n",
        "\n",
        "root = \"/content/dataset_extracted\"\n",
        "dataset_dir = find_dataset_dir(root)\n",
        "if dataset_dir is None:\n",
        "\n",
        "    if os.path.exists(\"/content/dataset\"):\n",
        "        dataset_dir = \"/content/dataset\"\n",
        "    else:\n",
        "        print(\"Could not auto-find dataset folder. Listing extracted content for debugging:\")\n",
        "        !ls -R /content/dataset_extracted\n",
        "        raise RuntimeError(\"Dataset folder not found automatically. Inspect the listing above.\")\n",
        "else:\n",
        "    print(\"Detected dataset directory:\", dataset_dir)\n",
        "\n",
        "final_dataset = \"/content/dataset\"\n",
        "\n",
        "if os.path.exists(final_dataset):\n",
        "    print(\"Cleaning existing /content/dataset ...\")\n",
        "    shutil.rmtree(final_dataset)\n",
        "os.makedirs(final_dataset, exist_ok=True)\n",
        "\n",
        "\n",
        "moved = 0\n",
        "for entry in os.listdir(dataset_dir):\n",
        "    s = os.path.join(dataset_dir, entry)\n",
        "    if os.path.isdir(s):\n",
        "\n",
        "        shutil.move(s, final_dataset)\n",
        "        moved += 1\n",
        "\n",
        "if moved == 0:\n",
        "    raise RuntimeError(f\"No class folders found under {dataset_dir} to move into {final_dataset}.\")\n",
        "print(f\"Moved {moved} class folders into {final_dataset}\")\n",
        "\n",
        "print(\"Final /content/dataset contents:\")\n",
        "!ls -1 /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVWMki3pDoW-",
        "outputId": "0bfc5e65-aa2a-4830-a787-1b1706c1f08a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "DATASET_PATH = \"/content/dataset\"\n",
        "print(\"Using dataset path:\", DATASET_PATH)\n",
        "print(\"Top-level folders (classes):\", sorted(os.listdir(DATASET_PATH)))\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH = 32\n",
        "VAL_SPLIT = 0.15\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=18,\n",
        "    zoom_range=0.12,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=VAL_SPLIT\n",
        ")\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        "    class_mode='sparse',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        "    class_mode='sparse',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "class_indices = train_gen.class_indices\n",
        "num_classes = len(class_indices)\n",
        "print(\"Detected classes (name -> index):\", class_indices)\n",
        "print(\"Number of classes:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "collapsed": true,
        "id": "TPdwPeaAD1qu",
        "outputId": "dd0f9a2b-fca7-4ee6-bacd-fc19fe60152b"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
        "\n",
        "base = VGG16(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE,3))\n",
        "base.trainable = False  # freeze weights\n",
        "\n",
        "x = base.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "out = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base.input, outputs=out)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GyZ2vnjqD89m",
        "outputId": "20da530b-a8b7-411f-fa38-d81e6c12678c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "]\n",
        "\n",
        "EPOCHS = 8\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qHNR8BC7GhU2",
        "outputId": "bceb369b-9f07-4f96-cd85-40ed3e27245c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend(); plt.title(\"Loss\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.legend(); plt.title(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "val_steps = int(np.ceil(val_gen.samples / val_gen.batch_size))\n",
        "y_true = val_gen.classes\n",
        "y_probs = model.predict(val_gen, steps=val_steps, verbose=1)\n",
        "y_pred = np.argmax(y_probs, axis=1)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "labels = [k for k,v in sorted(class_indices.items(), key=lambda item: item[1])]\n",
        "print(\"Labels (index order):\", labels)\n",
        "plt.figure(figsize=(8,6))\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=labels, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mpSAejlqGyYX",
        "outputId": "afee6de6-d801-4b3b-df47-90d53a7dea51"
      },
      "outputs": [],
      "source": [
        "\n",
        "index_to_disease = {v: k.strip() for k,v in class_indices.items()}\n",
        "print(\"Index -> Disease mapping:\", index_to_disease)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGJcs6LtM1mQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "doctor_data = [\n",
        "    {\"name\": \"Dr. Mehta\", \"specialization\": \"Ophthalmologist\", \"experience\": 15, \"fee\": 800, \"success_rate\": 96},\n",
        "    {\"name\": \"Dr. Sharma\", \"specialization\": \"Eye Surgeon\", \"experience\": 12, \"fee\": 900, \"success_rate\": 92},\n",
        "    {\"name\": \"Dr. Nair\", \"specialization\": \"Optometrist\", \"experience\": 10, \"fee\": 600, \"success_rate\": 88},\n",
        "    {\"name\": \"Dr. Khan\", \"specialization\": \"Retina Specialist\", \"experience\": 14, \"fee\": 1000, \"success_rate\": 95},\n",
        "    {\"name\": \"Dr. Gupta\", \"specialization\": \"Ophthalmologist\", \"experience\": 8, \"fee\": 700, \"success_rate\": 85},\n",
        "]\n",
        "\n",
        "disease_specialization = {\n",
        "    \"Bulging Eyes\": \"Ophthalmologist\",\n",
        "    \"Cataracts\": \"Eye Surgeon\",\n",
        "    \"Crossed Eyes\": \"Optometrist\",\n",
        "    \"Glaucoma\": \"Retina Specialist\",\n",
        "    \"Uveitis\": \"Ophthalmologist\",\n",
        "}\n",
        "\n",
        "def recommend_doctors_for_disease(disease, top_k=3):\n",
        "    if disease not in disease_specialization:\n",
        "        print(f\"Error: Disease '{disease}' not found in mapping!\")\n",
        "        return None, []\n",
        "\n",
        "    specialization = disease_specialization[disease]\n",
        "    doctors = [d for d in doctor_data if d[\"specialization\"] == specialization]\n",
        "    sorted_docs = sorted(doctors, key=lambda x: (x[\"success_rate\"], x[\"experience\"]), reverse=True)\n",
        "    return specialization, sorted_docs[:top_k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrAm74CcNII2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "index_to_disease = {\n",
        "    0: \"Bulging Eyes\",\n",
        "    1: \"Cataracts\",\n",
        "    2: \"Crossed Eyes\",\n",
        "    3: \"Glaucoma\",\n",
        "    4: \"Uveitis\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "collapsed": true,
        "id": "Q9buz9nUM5S2",
        "outputId": "17e96cbd-8c3e-48c0-ba05-0b235116c65d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "print(\"Upload an eye image to test the pipeline (jpg/png).\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"No image uploaded. You can also test using validation images (val_gen.filepaths).\")\n",
        "else:\n",
        "    img_name = list(uploaded.keys())[0]\n",
        "    img_path = img_name\n",
        "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    arr = image.img_to_array(img) / 255.0\n",
        "    arr = np.expand_dims(arr, 0)\n",
        "    preds = model.predict(arr)\n",
        "    idx = int(np.argmax(preds, axis=1)[0])\n",
        "    disease = index_to_disease.get(idx, \"Unknown\").strip()\n",
        "    prob = float(np.max(preds))\n",
        "    print(f\"Predicted disease: {disease}  (confidence: {prob:.3f})\")\n",
        "\n",
        "    spec, top_docs = recommend_doctors_for_disease(disease, top_k=3)\n",
        "    print(\"Mapped specialization:\", spec)\n",
        "    print(\"Top recommended doctors:\")\n",
        "    for d in top_docs:\n",
        "        print(f\" - {d['name']} ({d['specialization']}), Exp: {d['experience']} yrs, Fees: ₹{d['fee']}, Success: {d['success_rate']}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
